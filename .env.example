# Attest Framework — Environment Configuration
# Copy to .env and fill in your values: cp .env.example .env

# ── Provider API Keys ──
# At minimum, set ATTEST_OPENAI_API_KEY to enable Layers 5-6.
ATTEST_OPENAI_API_KEY=
ATTEST_ANTHROPIC_API_KEY=
ATTEST_GEMINI_API_KEY=

# Ollama requires no key — just the base URL.
ATTEST_OLLAMA_BASE_URL=http://localhost:11434

# ── Embedding (Layer 5) ──
# Provider: "auto" (uses OpenAI if key present), "openai", or "onnx"
ATTEST_EMBEDDING_PROVIDER=auto

# ── LLM Judge (Layer 6) ──
# Provider: "openai", "anthropic", "gemini", "ollama"
# Falls back through providers in that order if unset.
ATTEST_JUDGE_PROVIDER=
# Override the default judge model (provider-specific).
ATTEST_JUDGE_MODEL=

# ── Cache ──
ATTEST_CACHE_DIR=~/.attest/cache
ATTEST_EMBEDDING_CACHE_MAX_MB=500

# ── ONNX Local Embedding (optional, requires onnx build tag) ──
ATTEST_MODEL_DIR=~/.attest/models

# ── Rate Limiting ──
# Requests per minute per provider.
ATTEST_RATE_LIMIT_OPENAI=60
ATTEST_RATE_LIMIT_ANTHROPIC=60
ATTEST_RATE_LIMIT_GEMINI=60
